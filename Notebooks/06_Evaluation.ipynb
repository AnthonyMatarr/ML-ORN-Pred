{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed292ec4",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1316fb",
   "metadata": {},
   "source": [
    "Import libraries/packages + preprocessed data + models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81c341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.build_dnn_model import build_nn_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn import metrics, calibration\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from src.config import SEED\n",
    "\n",
    "## Data\n",
    "y_train, y_test = (pd.read_excel('../data/raw/split/Raw_y_train.xlsx'))['ORN'], (pd.read_excel('../data/raw/split/Raw_y_test.xlsx'))['ORN']\n",
    "ml_X_train = pd.read_parquet('../data/processed/ml_train_transformed.parquet')\n",
    "ml_X_test = pd.read_parquet('../data/processed/ml_test_transformed.parquet')\n",
    "nomo_X_train = pd.read_parquet('../data/processed/nomo_train_transformed.parquet')\n",
    "nomo_X_test = pd.read_parquet('../data/processed/nomo_test_transformed.parquet')\n",
    "\n",
    "## Models\n",
    "lightgbm_clf = joblib.load('../models/LightGBM.joblib')\n",
    "svc_clf = joblib.load('../models/SVC.joblib')\n",
    "knn_clf = joblib.load('../models/KNN.joblib')\n",
    "dnn_clf = joblib.load('../models/DNN.joblib')\n",
    "stack_clf = joblib.load('../models/stack.joblib')\n",
    "nomo_clf = joblib.load('../models/NLR.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47bb9f7",
   "metadata": {},
   "source": [
    "Evaluation functions (split for readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7773f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report_df = {}\n",
    "\n",
    "def get_cm(model_name, y_true, y_pred, data_type, save_plot = False):\n",
    "    cm = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt ='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.title(f'{model_name}: {data_type}')\n",
    "    if save_plot and data_type == 'Testing':\n",
    "        plt.savefig(f'../results/figures/CM/{model_name}_CM.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def get_auc_CI(y_true, y_pred, curve_type, n_bootstraps=3000, seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    classes, counts = np.unique(y_true, return_counts=True)\n",
    "    class_indices = [np.where(y_true == cl)[0] for cl in classes]\n",
    "    \n",
    "    scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_indices = np.concatenate([ np.random.choice(idx, size=count, replace=True) for idx, count in zip(class_indices, counts)])\n",
    "        if curve_type == 'roc':\n",
    "            scores.append(metrics.roc_auc_score(y_true[sample_indices], y_pred[sample_indices]))\n",
    "        elif curve_type == 'prc':\n",
    "            precision, recall, _ = metrics.precision_recall_curve(y_true[sample_indices], y_pred[sample_indices])\n",
    "            scores.append(metrics.auc(recall, precision))\n",
    "    \n",
    "    return (\n",
    "        np.percentile(scores, 2.5),\n",
    "        np.percentile(scores, 97.5)\n",
    "        )\n",
    "\n",
    "def get_class_report_CI(y_true, y_pred, n_bootstraps=3000, seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    classes, counts = np.unique(y_true, return_counts=True)\n",
    "    class_indices = [np.where(y_true == cl)[0] for cl in classes]\n",
    "    \n",
    "    all_boot_metrics = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_indices = np.concatenate([ \n",
    "            np.random.choice(idx, size=count, replace=True) \n",
    "            for idx, count in zip(class_indices, counts)])\n",
    "        # Calculate metrics\n",
    "        y_true_boot = y_true[sample_indices]\n",
    "        y_pred_boot = y_pred[sample_indices]\n",
    "        single_boot_metric = get_discrimination_metrics(y_true_boot, y_pred_boot)\n",
    "\n",
    "        all_boot_metrics.append((single_boot_metric['Accuracy'], \n",
    "                                 single_boot_metric['Precision'],\n",
    "                                 single_boot_metric['F1-Score'],\n",
    "                                 single_boot_metric['Sensitivity'], \n",
    "                                 single_boot_metric['Specificity'], \n",
    "                                 single_boot_metric['MCC']))\n",
    "\n",
    "    all_boot_metrics = np.array(all_boot_metrics)\n",
    "    ci_low = np.percentile(all_boot_metrics, 2.5, axis=0)\n",
    "    ci_high = np.percentile(all_boot_metrics, 97.5, axis=0)\n",
    "    true_metrics = get_discrimination_metrics(y_true, y_pred)\n",
    "    return {\n",
    "        'Accuracy': f'{true_metrics[\"Accuracy\"]:.3f} ({ci_low[0]:.3f}-{ci_high[0]:.3f})',\n",
    "        'Precision': f'{true_metrics[\"Precision\"]:.3f} ({ci_low[1]:.3f}-{ci_high[1]:.3f})',\n",
    "        'F1-Score': f'{true_metrics[\"F1-Score\"]:.3f} ({ci_low[2]:.3f}-{ci_high[2]:.3f})',\n",
    "        'Sensitivity': f'{true_metrics[\"Sensitivity\"]:.3f} ({ci_low[3]:.3f}-{ci_high[3]:.3f})',\n",
    "        'Specificity': f'{true_metrics[\"Specificity\"]:.3f} ({ci_low[4]:.3f}-{ci_high[4]:.3f})',\n",
    "        'MCC': f'{true_metrics[\"MCC\"]:.3f} ({ci_low[5]:.3f}-{ci_high[5]:.3f})'\n",
    "    }\n",
    "  \n",
    "def get_callibration_CI(y_pred_proba, y_true, n_bootstraps=3000, seed = SEED):\n",
    "    np.random.seed(seed)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    classes, counts = np.unique(y_true, return_counts=True)\n",
    "    class_indices = [np.where(y_true == cl)[0] for cl in classes]\n",
    "    \n",
    "    brier_scores = []\n",
    "    ici_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_indices = np.concatenate([\n",
    "            np.random.choice(idx, size=count, replace=True) \n",
    "            for idx, count in zip(class_indices, counts)\n",
    "            ])\n",
    "        y_true_boot = y_true[sample_indices]\n",
    "        y_proba_boot = y_pred_proba[sample_indices]\n",
    "\n",
    "        # Brier\n",
    "        brier = metrics.brier_score_loss(y_true_boot, y_proba_boot)\n",
    "        brier_scores.append(brier)\n",
    "        #ICI\n",
    "        prob_true, prob_pred = calibration.calibration_curve(y_true_boot, y_proba_boot, n_bins=3, strategy='uniform')\n",
    "        ici = np.mean(np.abs(prob_true - prob_pred))\n",
    "        ici_scores.append(ici)\n",
    "\n",
    "\n",
    "    brier_ci = [\n",
    "        np.percentile(brier_scores, 2.5),\n",
    "        np.percentile(brier_scores, 97.5),\n",
    "        ]\n",
    "    ici_ci = [\n",
    "        np.percentile(ici_scores, 2.5),\n",
    "        np.percentile(ici_scores, 97.5),\n",
    "        ]\n",
    "\n",
    "    return brier_ci, ici_ci\n",
    "\n",
    "def get_bin_stats(model, model_name, X_test, y_test, save_plot, low_thr = 0.33, high_thr = 0.66):\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    df = pd.DataFrame({'y_true': y_test, 'y_pred_proba': y_pred_proba})\n",
    "    bins = [0.0, low_thr, high_thr, 1.0]\n",
    "    bin_labels = ['low', 'medium', 'high']\n",
    "    df['risk_bin'] = pd.cut(df['y_pred_proba'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "\n",
    "    bin_stats = df.groupby('risk_bin').agg(\n",
    "        N_patients = ('y_true', 'size'),\n",
    "        Percent = ('y_true', lambda x: 100 * len(x) / len(df)),\n",
    "        Observed_Event_Rate = ('y_true', 'mean'),\n",
    "        Avg_Pred_Prob = ('y_pred_proba', 'mean'),\n",
    "        Positives_in_bin = ('y_true', 'sum')\n",
    "    )\n",
    "    total_positives = df['y_true'].sum()\n",
    "    bin_stats['Percent_of_All_Positives'] = 100 * bin_stats['Positives_in_bin'] / total_positives\n",
    "    print(bin_stats)\n",
    "\n",
    "    plt.bar(bin_stats.index, bin_stats['Observed_Event_Rate'], alpha=0.7, label='Observed')\n",
    "    plt.plot(bin_stats.index, bin_stats['Avg_Pred_Prob'], marker='o', color='red', label='Predicted')\n",
    "    plt.ylabel('Event Rate')\n",
    "    plt.xlabel('Risk Bin')\n",
    "    plt.title(f'Proportions of Average Observed vs Predicted ORN Occurrence: {model_name}')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.0)\n",
    "    if save_plot:\n",
    "        plt.savefig(f'../results/figures/calibration/bars/{model_name}_CalBar.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def get_discrimination_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    sensitivity = tp / (tp + fn) \n",
    "    specificity = tn / (tn + fp) \n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'F1-Score': f1,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'MCC': mcc\n",
    "    }\n",
    "\n",
    "def plot_ROC(y_true, y_prob, data_type, n_bootstraps=3000, seed=SEED): ## Will return optimal prediction threshold\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    lower_CI, upper_CI = get_auc_CI(y_true, y_prob, 'roc', n_bootstraps, seed)\n",
    "    auroc_string = f'{roc_auc:.3f} ({lower_CI:.3f}-{upper_CI:.3f})'\n",
    "    model_score = f'AUROC = {auroc_string}'\n",
    "    \n",
    "    pr_dif = tpr-fpr\n",
    "    optimal_idx = np.argmax(pr_dif)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    plt.plot(fpr, tpr, lw=4, label=f'{data_type} {model_score}')\n",
    "    return auroc_string, optimal_threshold\n",
    "\n",
    "def plot_calibration(y_pred_proba, y_true, data_type = 'Train', n_bootstraps = 3000, seed = SEED):\n",
    "    cal_info = {}\n",
    "    brier = metrics.brier_score_loss(y_true, y_pred_proba)\n",
    "    prob_true, prob_pred = calibration.calibration_curve(y_true, y_pred_proba, n_bins=3, strategy='uniform')\n",
    "    ici = np.mean(np.abs(prob_true - prob_pred))\n",
    "    brier_list, ici_list = get_callibration_CI(y_pred_proba, y_true, n_bootstraps, seed)\n",
    "    brier_lower_ci = brier_list[0]\n",
    "    brier_upper_ci= brier_list[1]\n",
    "    ici_lower_ci = ici_list[0]\n",
    "    ici_upper_ci = ici_list[1]\n",
    "    if data_type == \"Train\":\n",
    "        marker = 'o'\n",
    "    else:\n",
    "        marker = 's'\n",
    "        cal_info = {'Brier': brier,\n",
    "                    'Brier_low_CI': brier_lower_ci,\n",
    "                    'Brier_high_CI': brier_upper_ci,\n",
    "                    'ICI': ici,\n",
    "                    'ICI_low_CI': ici_lower_ci,\n",
    "                    'ICI_high_CI': ici_upper_ci}\n",
    "    plt.plot(prob_pred, prob_true, marker=marker, \n",
    "             label=f'{data_type} Brier = {brier:.3f} ({brier_lower_ci:.3f}-{brier_upper_ci:.3f}) & ICI = {ici:.3f} ({ici_lower_ci:.3f}-{ici_upper_ci:.3f})')\n",
    "    return cal_info\n",
    "\n",
    "def evaluate(model, model_name, X_train, y_train, X_test, y_test, \n",
    "             include_training = True, include_test = True, save_results = False):   \n",
    "    class_report_df[model_name] = {}\n",
    "    ########## Get prediction probabilities and hard predictions based on threshold ########## \n",
    "    ## Probabilities\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:, 1] if include_test else None\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1] if include_training else None\n",
    "\n",
    "    ##################### ROC PLOT AND AUROC #########################\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')\n",
    "    ##Training##\n",
    "    _, estimated_threshold = plot_ROC(y_train, y_pred_proba_train, 'Training')\n",
    "    threshold =  estimated_threshold\n",
    "\n",
    "    ##Testing##\n",
    "    if include_test:\n",
    "        auroc_string, _ = plot_ROC(y_test, y_pred_proba_test, 'Testing')\n",
    "        \n",
    "    ####### Plot graphs ######\n",
    "    #plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 21, fontweight = 550)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 21, fontweight = 550)\n",
    "    plt.tick_params(axis = 'both', which = 'major', labelsize=15)\n",
    "    plt.title(f'{model_name} ROC', fontweight='semibold', fontsize = 25)\n",
    "    plt.legend(loc=\"lower right\", prop = {'size': 19, 'weight': 550})\n",
    "    if save_results:\n",
    "        plt.savefig(f'../results/figures/ROC/{model_name}_ROC.pdf', bbox_inches='tight')\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "    #### Hard predictions + CM + classification report ####\n",
    "    if include_training: \n",
    "        y_pred_train = (y_pred_proba_train >= threshold).astype('int')\n",
    "        get_cm(model_name, y_train, y_pred_train, 'Training', save_results)\n",
    "    if include_test: \n",
    "        #Get CM\n",
    "        y_pred_test = (y_pred_proba_test >= threshold).astype('int')\n",
    "        get_cm(model_name, y_test, y_pred_test, 'Testing', save_results)\n",
    "        ##Classification report just to print out\n",
    "        class_report = metrics.classification_report(y_test, y_pred_test) \n",
    "        print(class_report)\n",
    "        ##Further reporting to add to class report\n",
    "        boot_class_report = get_class_report_CI(y_test, y_pred_test)\n",
    "        class_report_df[model_name] = boot_class_report\n",
    "        #Add AUROC to class_report\n",
    "        class_report_df[model_name]['AUROC (95% CI)'] = auroc_string\n",
    "        \n",
    "\n",
    "    ##################### CALIBRATION #########################\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "    plot_calibration(y_pred_proba_train, y_train, data_type='Train')\n",
    "    if include_test:\n",
    "        calibration_info = plot_calibration(y_pred_proba_test, y_test, data_type='Test')\n",
    "        class_report_df[model_name]['Brier Score (95% CI)'] = f'{calibration_info[\"Brier\"]:.3f} ({calibration_info[\"Brier_low_CI\"]:.3f}-{calibration_info[\"Brier_high_CI\"]:.3f})'\n",
    "        class_report_df[model_name]['Integrated Calibration Index (ICI) (95% CI)'] = f'{calibration_info[\"ICI\"]:.3f} ({calibration_info[\"ICI_low_CI\"]:.3f}-{calibration_info[\"ICI_high_CI\"]:.3f})'\n",
    "        class_report_df[model_name]['Threshold'] = np.round(threshold, 3)\n",
    "        \n",
    "    \n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'{model_name}')\n",
    "    plt.legend(loc='upper left', prop = {'size': 15, 'weight': 550})\n",
    "    plt.grid(True)\n",
    "    if save_results:\n",
    "        plt.savefig(f'../results/figures/calibration/curves/{model_name}_CalCurve.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    if include_test:\n",
    "        get_bin_stats(model, model_name, X_test, y_test, save_results, low_thr = 0.33, high_thr = 0.66)\n",
    "    print(f'Threshold used for hard predictions: {threshold:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3c3f0",
   "metadata": {},
   "source": [
    "Get ROC, CM, and discrimination values for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d46751",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    'LightGBM': lightgbm_clf,\n",
    "    'SVC': svc_clf,\n",
    "    'KNN': knn_clf,\n",
    "    'DNN': dnn_clf,\n",
    "    'Stack': stack_clf\n",
    "}\n",
    "for model_name, model in base_models.items():\n",
    "    evaluate(model, model_name, \n",
    "             ml_X_train, y_train, \n",
    "             ml_X_test, y_test,\n",
    "             save_results=True)\n",
    "    \n",
    "evaluate(nomo_clf, 'LR-Nomogram',\n",
    "         nomo_X_train, y_train,\n",
    "         nomo_X_test, y_test)\n",
    "\n",
    "class_report_df_saved = pd.DataFrame(class_report_df).T\n",
    "display(class_report_df_saved)\n",
    "class_report_df_saved.to_excel('../results/tables/class_report.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a30371",
   "metadata": {},
   "source": [
    "# AUROC P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['R_HOME'] = '/Library/Frameworks/R.framework/Resources'\n",
    "%reload_ext rpy2.ipython\n",
    "\n",
    "##Y test true\n",
    "y_true = y_test.to_list()\n",
    "%R -i y_true\n",
    "\n",
    "##NOTE: predict_proba() returns 2-d array, second entry is predicted prob of ORN occurance\n",
    "#LightGBM\n",
    "y_proba_light = lightgbm_clf.predict_proba(ml_X_test)[:,1]\n",
    "%R -i y_proba_light\n",
    "#SVC\n",
    "y_proba_svc = svc_clf.predict_proba(ml_X_test)[:,1]\n",
    "%R -i y_proba_svc\n",
    "#KNN\n",
    "y_proba_knn = knn_clf.predict_proba(ml_X_test)[:,1]\n",
    "%R -i y_proba_knn\n",
    "#DNN\n",
    "y_proba_nn = dnn_clf.predict_proba(ml_X_test)[:,1]\n",
    "%R -i y_proba_nn\n",
    "#Stack\n",
    "y_proba_stack = stack_clf.predict_proba(ml_X_test)[:,1]\n",
    "%R -i y_proba_stack\n",
    "\n",
    "#Nomogram (LR)\n",
    "y_proba_nomo = nomo_clf.predict_proba(nomo_X_test)[:,1]\n",
    "%R -i y_proba_nomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(pROC)\n",
    "get_pval <- function(y_true, y_proba_nomo, model_pred) {\n",
    "  roc_baseline <- roc(y_true, y_proba_nomo)\n",
    "  roc_model <- roc(y_true, model_pred)\n",
    "  test_result <- roc.test(roc_model, roc_baseline, method = 'delong')\n",
    "  return(test_result)\n",
    "}\n",
    "light_pval = get_pval(y_true, y_proba_nomo, y_proba_light)\n",
    "svc_pval = get_pval(y_true, y_proba_nomo, y_proba_svc)\n",
    "knn_pval = get_pval(y_true, y_proba_nomo, y_proba_knn)\n",
    "nn_pval = get_pval(y_true, y_proba_nomo, y_proba_nn)\n",
    "stack_pval = get_pval(y_true, y_proba_nomo, y_proba_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a64266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o roc_sig_df\n",
    "# Helper function to extract elements from roc.test objects\n",
    "extract_pval_stats <- function(test_obj) {\n",
    "  list(\n",
    "    P_Value = round(test_obj$p.value, 4),\n",
    "    Z_Statistic = round(test_obj$statistic, 4),\n",
    "    AUC_Model = round(test_obj$estimate[1], 4),\n",
    "    AUC_Baseline = round(test_obj$estimate[2], 4),\n",
    "    CI_Lower = round(test_obj$conf.int[1], 4),\n",
    "    CI_Upper = round(test_obj$conf.int[2], 4)\n",
    "  )\n",
    "}\n",
    "\n",
    "# List of all test objects\n",
    "pval_objects <- list(light_pval, svc_pval, knn_pval, nn_pval, stack_pval)\n",
    "\n",
    "# Apply extraction function to all objects\n",
    "extracted_data <- lapply(pval_objects, extract_pval_stats)\n",
    "\n",
    "# Convert to data frame and add model names\n",
    "roc_sig_df <- do.call(rbind, lapply(extracted_data, data.frame))\n",
    "roc_sig_df$Model <- c(\"LightGBM\", \"SVC\", \"KNN\", \"Deep Neural Network\", \"Stack\")\n",
    "\n",
    "# Reorder columns for clarity\n",
    "roc_sig_df <- roc_sig_df[, c(\"Model\", \"P_Value\", \"Z_Statistic\",\n",
    "                            \"AUC_Model\", \"AUC_Baseline\",\n",
    "                            \"CI_Lower\", \"CI_Upper\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(roc_sig_df)\n",
    "roc_sig_df.to_excel('../results/tables/roc_sig_table.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d23c7",
   "metadata": {},
   "source": [
    "# Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = '../results/tables/all_models_bin_info.xlsx'\n",
    "if os.path.exists(excel_path):\n",
    "    os.remove(excel_path)\n",
    "\n",
    "y_scores = {}\n",
    "y_scores['LR-Nomogram'] = model.predict_proba(nomo_X_test)[:, 1]\n",
    "for model_name, model in base_models.items():\n",
    "    y_scores[model_name] = model.predict_proba(ml_X_test)[:, 1]\n",
    "\n",
    "for model_name, y_pred_proba in y_scores.items():\n",
    "    print(f'--------------------- {model_name} ---------------------')\n",
    "    ## Get actual positive rate and mean predicted probability\n",
    "    prob_true, prob_pred = calibration.calibration_curve(y_test, y_pred_proba, n_bins=3, strategy='uniform')\n",
    "\n",
    "    ## Put predictions into bins\n",
    "    df = pd.DataFrame({'y_pred_proba': y_pred_proba, 'y_true': y_test})\n",
    "    df = df.sort_values(by='y_pred_proba').reset_index(drop=True)\n",
    "    bin_edges = np.linspace(0, 1,  4)\n",
    "    bin_labels = range(len(bin_edges)-1)\n",
    "    df['bin'] = pd.cut(df['y_pred_proba'], bins=bin_edges, include_lowest=True, labels=bin_labels)\n",
    "    df['bin'] = pd.Categorical(df['bin'], categories=bin_labels)\n",
    "    ## Create table\n",
    "    calibration_table = df.groupby('bin', observed=False).agg(\n",
    "        num_patients=('y_true', 'size'),\n",
    "        mean_predicted_prob= ('y_pred_proba', 'mean'),\n",
    "        actual_positive_rate=('y_true', 'mean')\n",
    "    ).reset_index()\n",
    "    ##Round\n",
    "    calibration_table['mean_predicted_prob'] = calibration_table['mean_predicted_prob'].round(3)\n",
    "    calibration_table['actual_positive_rate'] = calibration_table['actual_positive_rate'].round(3)\n",
    "    ##Fill NA for models w/ bins w/o any allocated patients\n",
    "    calibration_table['mean_predicted_prob'] = calibration_table['mean_predicted_prob'].fillna(\"None assigned\")\n",
    "    calibration_table['actual_positive_rate'] = calibration_table['actual_positive_rate'].fillna(\"None assigned\")\n",
    "    # Assign the appropriate bin range to each row (by bin index)\n",
    "    bin_range_labels = []\n",
    "    for i in range(len(bin_edges)-1):\n",
    "        if i == 0:\n",
    "            # First bin: closed on left\n",
    "            s = f\"[{bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}]\"\n",
    "        else:\n",
    "            # Subsequent bins: open on left\n",
    "            s = f\"({bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}]\"\n",
    "        bin_range_labels.append(s)\n",
    "    calibration_table['bin_range'] = calibration_table['bin'].apply(lambda i: bin_range_labels[i])\n",
    "    display(calibration_table)\n",
    "    ##Export\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a' if os.path.exists(excel_path) else 'w') as writer:\n",
    "        calibration_table.to_excel(writer, sheet_name=model_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
