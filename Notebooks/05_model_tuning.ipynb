{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b80bcec",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c3497",
   "metadata": {},
   "source": [
    "Import libraries/packages + preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472e996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.custom_transformers import remove_prefix\n",
    "from src.build_dnn_model import build_nn_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, ensemble, linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "##Neural Network\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from keras import layers, optimizers, metrics, callbacks, losses, initializers, Sequential, regularizers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from src.config import SEED\n",
    "\n",
    "##Raw (not-processed) training data\n",
    "raw_X_train, raw_y_train = pd.read_parquet('../data/raw/split/Raw_X_train.parquet'), pd.read_excel('../data/raw/split/Raw_y_train.xlsx', index_col=0)\n",
    "## Processed training data\n",
    "ml_X_train = pd.read_parquet('../data/processed/ml_train_transformed.parquet')\n",
    "ml_X_test = pd.read_parquet('../data/processed/ml_test_transformed.parquet')\n",
    "nomo_X_train = pd.read_parquet('../data/processed/nomo_train_transformed.parquet')\n",
    "##Pipeline\n",
    "ml_pipeline = joblib.load('../data/processed/ml_preprocessing_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76a9bd",
   "metadata": {},
   "source": [
    "# RF, LightGBM, SVC, KNN, LR-Nomogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3818248",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Param Search Grid\n",
    "model_params = {\n",
    "    'RF': {\n",
    "        'model': ensemble.RandomForestClassifier(random_state=SEED), \n",
    "        'params': {\n",
    "            'n_estimators': np.arange(5, 500, 10),\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'max_depth': np.arange(3, 50, 3),\n",
    "            'min_samples_split': np.arange(2, 15, 1),\n",
    "            'min_samples_leaf': np.arange(1, 15, 1),\n",
    "            'min_weight_fraction_leaf': np.linspace(0, 0.5, 10),\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'max_leaf_nodes': np.arange(3, 50, 5),\n",
    "            'min_impurity_decrease': np.linspace(0, 3, 15),\n",
    "            'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "            'max_samples': np.linspace(0.01, 1, 15)\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',  # Handles class imbalance automatically\n",
    "            verbosity=-1\n",
    "            ),\n",
    "        'params': {\n",
    "            'num_leaves': np.arange(5, 105, 10),\n",
    "            'min_data_in_leaf': np.arange(50, 500, 25),\n",
    "            'max_depth': [3, 5, 7, 10, -1],  # -1 means no limit\n",
    "\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            \n",
    "            \n",
    "            'min_child_samples': [5, 10, 20, 40],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 1],\n",
    "            'reg_lambda': [0, 0.1, 1],\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': svm.SVC(probability = True, random_state = SEED),\n",
    "        'params': {\n",
    "            'C': np.linspace(0.005, 10, 20),\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            #Only used for poly\n",
    "            'degree': [2,3,4,5,6,7,8],\n",
    "            #Only used for rbf, poly, sigmoid\n",
    "            'gamma': ['scale', 'auto'] + list(np.arange(0.05, 2, 10)),\n",
    "            #Onbly used for poly and sigmoid\n",
    "            'coef0': np.linspace(0.0, 1.5, 20),\n",
    "            'shrinking': [True, False],\n",
    "            'class_weight': ['balanced', None, {0:1, 1:1.5}, {0:1, 1:2}, {0:1, 1:3}, {0:1, 1:4}]\n",
    "        }\n",
    "    }, \n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3,4,5,6],\n",
    "            'weights': ['uniform', 'distance', None],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n",
    "            #only used for ball_tree or kd_tree\n",
    "            'leaf_size': np.arange(15, 45, 5),\n",
    "            'p': [1,2],\n",
    "            'metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'nan_euclidean', 'minkowski']\n",
    "        }\n",
    "    },     \n",
    "    'NLR': {\n",
    "        'model': linear_model.LogisticRegression(max_iter = 10000, random_state = SEED,\n",
    "                                                solver='liblinear'),\n",
    "        'params': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': np.linspace(0.01, 3, 50),\n",
    "            'class_weight': ['balanced', None, {0:1, 1: 1.15}, {0:1, 1: 1.25}, {0:1, 1: 1.5}, {0:1, 1: 1.75}, {0:1, 1: 2}, {0:1, 1: 2.5}, {0:1, 1: 3}],\n",
    "            #'l1_ratio': np.linspace(0.01,1, 20)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_w_random(X, y,\n",
    "                  model_name, n_iters, \n",
    "                  optimize_for, random_state = SEED,\n",
    "                  export_model = False):\n",
    "    model = model_params[model_name]['model']\n",
    "    params = model_params[model_name]['params']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=params,\n",
    "        n_iter= n_iters,    \n",
    "        scoring=optimize_for,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    best_params = search.best_params_\n",
    "    best_score = search.best_score_\n",
    "    print(f\"{model_name} Best Params: {best_params}\")\n",
    "    print(f\"{model_name} Best Average {optimize_for}: {best_score:.4f}\")\n",
    "    best_model = search.best_estimator_\n",
    "    if export_model:\n",
    "        joblib.dump(best_model, f'../models/{model_name}.joblib')\n",
    "    #Return best model\n",
    "    # NOTE: RandomSearchCV automatically retrains model on entire train set\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83266809",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model = tune_w_random(ml_X_train, raw_y_train.values.ravel(),\n",
    "                        'LightGBM', 500,\n",
    "                        'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07823aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = tune_w_random(ml_X_train, raw_y_train.values.ravel(),\n",
    "                        'SVC', 500,\n",
    "                        'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = tune_w_random(ml_X_train, raw_y_train.values.ravel(),\n",
    "                        'KNN', 500,\n",
    "                        'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomo_model = tune_w_random(nomo_X_train, raw_y_train.values.ravel(),\n",
    "                        'NLR', 500,\n",
    "                        'roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3c1ab",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f1691",
   "metadata": {},
   "source": [
    "Set up search space for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = raw_y_train.value_counts()[1] / raw_y_train.value_counts()[0] #Positive class ratio\n",
    "log_bias= np.log(pos / (1-pos))\n",
    "\n",
    "##Function used to build model (with search space) for keras-tuner\n",
    "def model_builder(hp):\n",
    "    hp_activation = hp.Choice('activation', values = ['relu', 'leaky_relu', 'elu']) #\n",
    "\n",
    "    #Layer size\n",
    "    hp_layer_1 = hp.Int('layer_1', min_value =5, max_value = 100, step = 5) # \n",
    "    hp_layer_2 = hp.Int('layer_2', min_value =5, max_value = 100, step = 5) # \n",
    "    #hp_layer_3 = hp.Int('layer_3', min_value =5, max_value = 100, step = 5)  # \n",
    "    #Dropout rate\n",
    "    hp_drop_1 = hp.Choice('drop_1', values = [0.25, 0.5, 0.75, 0.9, 0.99999]) # \n",
    "    hp_drop_2 = hp.Choice('drop_2', values = [0.25, 0.5, 0.75, 0.9, 0.99999]) #  \n",
    "    #hp_drop_3 = hp.Choice('drop_3', values = [0.25, 0.5, 0.75, 0.9, 0.99999]) #\n",
    "    # Regularization\n",
    "    hp_l2 = hp.Choice('l2', values=[0.0, 0.001, 0.01, 0.05, 0.1])\n",
    "    #Loss\n",
    "    hp_gamma = hp.Choice('gamma', values= [0.5, 1.0, 1.5, 2.0, 2.5]) #\n",
    "    hp_alpha = hp.Choice('alpha', values=[0.1, 0.15, 0.2, 0.25, 0.3]) #\n",
    "    hp_loss = losses.BinaryFocalCrossentropy(gamma=hp_gamma, alpha=hp_alpha)\n",
    "    #Optimizer\n",
    "    hp_lr = hp.Choice('learning_rate', values = [0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]) #\n",
    "    hp_momentum = hp.Choice('momentum', values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]) #\n",
    "\n",
    "    model = Sequential()\n",
    "    #Input layer\n",
    "    model.add(layers.Input(shape = ml_X_train.loc[0].shape))\n",
    "    #Hidden layer1\n",
    "    model.add(layers.Dense(units = hp_layer_1, \n",
    "                                    kernel_initializer= initializers.HeUniform(seed=SEED),\n",
    "                                    bias_initializer='zeros',\n",
    "                                    kernel_regularizer=regularizers.l2(hp_l2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(hp_activation))\n",
    "    model.add(layers.Dropout(hp_drop_1)) \n",
    "    #Hidden layer 2\n",
    "    model.add(layers.Dense(units = hp_layer_2, \n",
    "                                    kernel_initializer= initializers.HeUniform(seed=SEED),\n",
    "                                    bias_initializer='zeros',\n",
    "                                    kernel_regularizer=regularizers.l2(hp_l2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation(hp_activation))\n",
    "    model.add(layers.Dropout(hp_drop_2)) \n",
    "\n",
    "    # NOTE: Param code for third layer is commented out since tuning with 2 layers was explored after 3-layer tuning\n",
    "    ## Ultimately, 2 layers out-performed 3 layers in average AUROC across CV folds\n",
    "    ################################### Uncomment below for 3-layer tuning ###################################\n",
    "    #Hidden layer 3\n",
    "    #model.add(layers.Dense(units = hp_layer_3, \n",
    "    #                                kernel_initializer= initializers.HeUniform(seed=SEED),\n",
    "    #                                bias_initializer='zeros'))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.Activation(hp_activation))\n",
    "    #model.add(layers.Dropout(hp_drop_3)) \n",
    "    #########################################################################################################\n",
    "\n",
    "    #Output layer\n",
    "    model.add(layers.Dense(units = 1, activation='sigmoid',\n",
    "                                    kernel_initializer= initializers.GlorotUniform(seed=SEED),\n",
    "                                    bias_initializer= initializers.Constant(log_bias)\n",
    "                                    )\n",
    "                )\n",
    "    \n",
    "\n",
    "    model.compile(optimizers.SGD(learning_rate=hp_lr, momentum=hp_momentum),\n",
    "                  loss = hp_loss,\n",
    "                  metrics = [metrics.AUC(curve = 'ROC',name = 'AUCROC')])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679210f7",
   "metadata": {},
   "source": [
    "Tune architecture + hyper-parameters w/ stratified 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291abaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "all_best_scores = []\n",
    "all_best_hps = []\n",
    "\n",
    "i = 1\n",
    "for train_idx, val_idx in skf.split(raw_X_train, raw_y_train):\n",
    "    #Get raw train/val indices\n",
    "    X_train, X_val = raw_X_train.iloc[train_idx], raw_X_train.iloc[val_idx]\n",
    "    y_train, y_val = raw_y_train.iloc[train_idx], raw_y_train.iloc[val_idx]\n",
    "\n",
    "    train_transformed = ml_pipeline.fit_transform(X_train)\n",
    "    val_transformed = ml_pipeline.transform(X_val)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=model_builder,\n",
    "        objective=kt.Objective('val_AUCROC', 'max'),\n",
    "        max_epochs=200,\n",
    "        factor=3,\n",
    "        seed=SEED,\n",
    "        directory='../nn_tune_dir',\n",
    "        project_name=f'x_fold_{i}'\n",
    "    )\n",
    "\n",
    "    stop_early = callbacks.EarlyStopping(monitor='val_AUCROC', patience=3)\n",
    "    tuner.search(train_transformed, y_train,\n",
    "                 epochs=30,\n",
    "                 validation_data=(val_transformed, y_val),\n",
    "                 callbacks=[stop_early])\n",
    "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "    all_best_scores.append(best_trial.score)\n",
    "    all_best_hps.append(tuner.get_best_hyperparameters(1)[0])\n",
    "    i+=1\n",
    "\n",
    "mean_score = np.mean(all_best_scores)\n",
    "std_score = np.std(all_best_scores)\n",
    "print(f\"Mean CV val_AUCROC: {mean_score:.3f} Â± {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71389d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------- Best HPs for each fold --------------')\n",
    "for idx, hps in enumerate(all_best_hps):\n",
    "    print(f\"**************Fold {idx+1} best hyperparameters:**************\")\n",
    "    print(all_best_scores[idx])\n",
    "    print(hps.values)\n",
    "\n",
    "## Uncomment layer_3 and drop_3 lines for 3-layer tuning\n",
    "numeric_keys = [ 'layer_1',\n",
    " 'layer_2',\n",
    " #'layer_3',\n",
    " 'drop_1',\n",
    " 'drop_2',\n",
    " #'drop_3',\n",
    " 'l2',\n",
    " 'gamma',\n",
    " 'alpha',\n",
    " 'learning_rate',\n",
    " 'momentum']\n",
    "print('\\t\\t')\n",
    "print('-------------- Average (numerical) hyper-parameter values amongst best combinations --------------')\n",
    "average_hps = {}\n",
    "for key in numeric_keys:\n",
    "    values = [hps[key] for hps in all_best_hps if key in hps]\n",
    "    average_hps[key] = np.mean(values)\n",
    "\n",
    "for key in sorted(average_hps):\n",
    "    print(f\"{key}: {average_hps[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9347c5d3",
   "metadata": {},
   "source": [
    "Set final architecture + hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set final hyper-params\n",
    "chosen_hps = {'activation': 'relu', \n",
    " 'layer_1': 48, \n",
    " 'layer_2': 35, \n",
    " 'drop_1': 0.4, \n",
    " 'drop_2': 0.6, \n",
    " 'l2': 0.0022, \n",
    " 'gamma': 1.2, \n",
    " 'alpha': 0.22, \n",
    " 'learning_rate': 0.0970, \n",
    " 'momentum': 0.8, \n",
    " ## The last 5 params are arbitrary\n",
    " 'tuner/epochs': 100, \n",
    " 'tuner/initial_epoch': 20, \n",
    " 'tuner/bracket': 2, \n",
    " 'tuner/round': 2, \n",
    " 'tuner/trial_id': '0035'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554cd3f",
   "metadata": {},
   "source": [
    "Note: 82 epochs chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "all_val_metrics = []\n",
    "##NOTE: Iteratively \n",
    "min_epochs = 80\n",
    "max_epochs = 86\n",
    "step = 1\n",
    "\n",
    "i = 1\n",
    "for train_idx, val_idx in skf.split(raw_X_train, raw_y_train):\n",
    "    #Get raw train/val indices\n",
    "    X_train, X_val = raw_X_train.iloc[train_idx], raw_X_train.iloc[val_idx]\n",
    "    y_train, y_val = raw_y_train.iloc[train_idx], raw_y_train.iloc[val_idx]\n",
    "\n",
    "    train_transformed = ml_pipeline.fit_transform(X_train)\n",
    "    val_transformed = ml_pipeline.transform(X_val)\n",
    "\n",
    "    fold_val_metrics = []\n",
    "    for epoch in range(min_epochs, max_epochs, step):\n",
    "        model = KerasClassifier(\n",
    "            model = build_nn_model,\n",
    "            input_shape = ml_X_train.loc[0].shape,\n",
    "            hidden_layer_1 = chosen_hps.get('layer_1'),\n",
    "            hidden_layer_2 = chosen_hps.get('layer_2'),\n",
    "            #hidden_layer_3 = chosen_hps.get('layer_3'),\n",
    "            dropout_1 = chosen_hps.get('drop_1'),\n",
    "            dropout_2 = chosen_hps.get('drop_2'),\n",
    "            #dropout_3 = chosen_hps.get('drop_3'),\n",
    "            l2 = chosen_hps.get('l2'),\n",
    "            learning_rate = chosen_hps.get('learning_rate'),\n",
    "            #Log bias calculated earlier (few cells up)\n",
    "            log_bias = log_bias,\n",
    "            activation = chosen_hps.get('activation'),\n",
    "            momentum = chosen_hps.get('momentum'),\n",
    "            loss_func = losses.BinaryFocalCrossentropy(gamma=chosen_hps.get('gamma'), alpha=chosen_hps.get('alpha')),\n",
    "            seed = SEED, #model builder random state\n",
    "            epochs = epoch,\n",
    "            verbose = 0,\n",
    "            random_state= SEED #keras classifier random state\n",
    "        )\n",
    "        model.fit(train_transformed, y_train)\n",
    "        #Get score\n",
    "        y_pred_proba = model.predict_proba(val_transformed)[:, 1]\n",
    "        #y_pred_proba = get_y_pred_probability(model, 'NN', val_transformed)\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "        val_metric = auc(fpr, tpr)\n",
    "        fold_val_metrics.append(val_metric)\n",
    "    all_val_metrics.append(fold_val_metrics)\n",
    "    print(f'Done with fold {i}')\n",
    "    i+=1\n",
    "\n",
    "# Compute average validation metric per epoch\n",
    "avg_val_metrics = np.mean(all_val_metrics, axis=0)\n",
    "best_epoch = (np.argmax(avg_val_metrics) * step) + min_epochs\n",
    "\n",
    "best_score = np.max(avg_val_metrics)\n",
    "print(f\"Best CV val AUROC: {best_epoch} Epochs with score of {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40914247",
   "metadata": {},
   "source": [
    "Fit final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = KerasClassifier(\n",
    "   model = build_nn_model,\n",
    "   input_shape = ml_X_train.loc[0].shape,\n",
    "   hidden_layer_1 = chosen_hps.get('layer_1'),\n",
    "   hidden_layer_2 = chosen_hps.get('layer_2'),\n",
    "   #hidden_layer_3 = chosen_hps.get('layer_3'),\n",
    "   dropout_1 = chosen_hps.get('drop_1'),\n",
    "   dropout_2 = chosen_hps.get('drop_2'),\n",
    "   #dropout_3 = chosen_hps.get('drop_3'),\n",
    "   l2 = chosen_hps.get('l2'),\n",
    "   learning_rate = chosen_hps.get('learning_rate'),\n",
    "   log_bias = log_bias,\n",
    "   activation = chosen_hps.get('activation'),\n",
    "   momentum = chosen_hps.get('momentum'),\n",
    "   loss_func = losses.BinaryFocalCrossentropy(gamma=chosen_hps.get('gamma'), alpha=chosen_hps.get('alpha')),\n",
    "   seed = SEED,\n",
    "   epochs = best_epoch,\n",
    "   verbose = 1,\n",
    "   random_state= SEED\n",
    ")\n",
    "\n",
    "##Train model with tuned hyper params\n",
    "nn_model.fit(ml_X_train, raw_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed0cc6",
   "metadata": {},
   "source": [
    "Plot history and (optionally) export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb93fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = nn_model.history_\n",
    "\n",
    "###Plot AUROC and loss over epochs\n",
    "# AUROC\n",
    "train_auc_roc = history['AUCROC']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_auc_roc, label='Train AUC-ROC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('AUC-ROC Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Loss\n",
    "train_loss = history['loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(nn_model, '../models/DNN.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbba9c",
   "metadata": {},
   "source": [
    "# Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model = ensemble.StackingClassifier(estimators=[\n",
    "                                              ('LightGBM', lightgbm_model),\n",
    "                                              ('SVC', svc_model), \n",
    "                                              ('KNN', knn_model), \n",
    "                                              ('Neural Network', nn_model)\n",
    "                                              ],\n",
    "                                       cv =5,\n",
    "                                       final_estimator= linear_model.LogisticRegression()\n",
    "                                       )\n",
    "stack_model.fit(ml_X_train, raw_y_train.values.ravel())\n",
    "joblib.dump(stack_model, '../models/stack.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
