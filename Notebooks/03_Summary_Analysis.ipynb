{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a832a7",
   "metadata": {},
   "source": [
    "# Data Summary + Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2f8c1",
   "metadata": {},
   "source": [
    "Import libraries/packages + cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec580fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.feature_lists import get_feature_lists\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from scipy.stats import chi2_contingency, fisher_exact, mannwhitneyu\n",
    "from statsmodels.api import Logit\n",
    "from statsmodels.tools import add_constant\n",
    "import sigfig\n",
    "\n",
    "\n",
    "from src.config import SEED\n",
    "df = pd.read_parquet('../data/raw/Cleaned_ORN.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a078d1c",
   "metadata": {},
   "source": [
    "Classify features by data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imported func from src\n",
    "feature_lists = get_feature_lists()\n",
    "binary_cols = feature_lists[\"binary_cols\"]\n",
    "numerical_cols = feature_lists[\"numerical_cols\"]\n",
    "nominal_cols = feature_lists[\"nominal_cols\"]\n",
    "ordinal_cols = feature_lists[\"ordinal_cols\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897777ff",
   "metadata": {},
   "source": [
    "# Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### General Split#######\n",
    "#75-25 train-test split with stratified distribution of ORN to No-ORN\n",
    "raw_X_train, raw_X_test, raw_y_train, raw_y_test = train_test_split(df.drop(['ORN'], axis = 1),\n",
    "                                                    df['ORN'], \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = SEED, \n",
    "                                                    stratify=df.ORN\n",
    "                                                    )\n",
    "#Reset index\n",
    "raw_X_train.reset_index(drop = True, inplace=True)\n",
    "raw_y_train.reset_index(drop = True, inplace=True)\n",
    "raw_X_test.reset_index(drop = True, inplace=True)\n",
    "raw_y_test.reset_index(drop = True, inplace=True)\n",
    "#Export raw train/test splits\n",
    "raw_X_train.to_parquet('../data/raw/split/Raw_X_train.parquet')\n",
    "raw_X_test.to_parquet('../data/raw/split/Raw_X_test.parquet')\n",
    "raw_y_train.to_excel('../data/raw/split/Raw_y_train.xlsx', index = True)\n",
    "raw_y_test.to_excel('../data/raw/split/Raw_y_test.xlsx', index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full train: \", raw_y_train.value_counts())\n",
    "print(raw_X_train.shape)\n",
    "print(\"Full test: \", raw_y_test.value_counts())\n",
    "print(raw_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621eb2b6",
   "metadata": {},
   "source": [
    "# Summary across Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ec367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_column(df, ORN_type, all_categories):\n",
    "    total_entries = len(df)\n",
    "    header = f'{ORN_type} (n={total_entries})'\n",
    "    summary_list = []\n",
    "    for col in df:\n",
    "        if col == 'ORN':\n",
    "            continue\n",
    "        elif col in binary_cols + nominal_cols + ordinal_cols:\n",
    "            counts = df[col].value_counts(dropna=False)\n",
    "            percentages = np.round(df[col].value_counts(normalize=True, dropna=False) * 100, 1)\n",
    "            summary_list.append({\n",
    "                'Feature': col.upper(),\n",
    "                header: ''\n",
    "            })\n",
    "            for entry in all_categories[col]:\n",
    "                summary_list.append({\n",
    "                    'Feature': f'{col.upper()} {entry}',\n",
    "                    header: f'{counts.get(entry, 0)} ({percentages.get(entry, 0.0)})'\n",
    "                })\n",
    "            # Add missing value row\n",
    "            n_missing = df[col].isnull().sum()\n",
    "            pct_missing = np.round(n_missing / total_entries * 100, 1)\n",
    "            summary_list.append({\n",
    "                'Feature': f'{col.upper()} Missing',\n",
    "                header: f'{n_missing} ({pct_missing})'\n",
    "            })\n",
    "        else:\n",
    "            # Numerical: mean and SD on non-missing, plus missing count/%\n",
    "            non_missing = df[col].dropna()\n",
    "            avg = np.round(non_missing.mean(), 1)\n",
    "            std = np.round(non_missing.std(), 1)\n",
    "            n_missing = df[col].isnull().sum()\n",
    "            pct_missing = np.round(n_missing / total_entries * 100, 1)\n",
    "            summary_list.append({\n",
    "                'Feature': f'{col.upper()}, ± (SD)',\n",
    "                header: f'{avg} ± {std}'\n",
    "            })\n",
    "            summary_list.append({\n",
    "                'Feature': f'{col.upper()} Missing',\n",
    "                header: f'{n_missing} ({pct_missing})'\n",
    "            })\n",
    "    return pd.DataFrame(summary_list)\n",
    "\n",
    "def generate_summary_table(X_df, y_df,all_categories):\n",
    "    df = pd.concat([X_df, y_df], axis = 1) #Get merged df with ORN column\n",
    "    non_orn_df = df[df['ORN'] == 0] \n",
    "    orn_df = df[df['ORN'] == 1] \n",
    "    \n",
    "    #Get total, non-orn, and orn summary tables\n",
    "    total_col = generate_summary_column(df, 'Total', all_categories)\n",
    "    non_orn_col = generate_summary_column(non_orn_df, 'Non-ORN', all_categories)\n",
    "    orn_col = generate_summary_column(orn_df, 'ORN', all_categories)\n",
    "\n",
    "    #Drop feature column for readability when combined\n",
    "    orn_col.drop('Feature', axis = 1, inplace = True)\n",
    "    non_orn_col.drop('Feature', axis = 1, inplace = True)\n",
    "\n",
    "    #Combine total, non-orn, and orn columns\n",
    "    combined_df = pd.concat([total_col, non_orn_col], axis =1)\n",
    "    combined_df = pd.concat([combined_df, orn_col], axis =1)\n",
    "    return combined_df\n",
    "\n",
    "#### Get all unique entries for each categorical variable\n",
    "all_categories = {}\n",
    "for col in nominal_cols + binary_cols + ordinal_cols:\n",
    "    train_categories = raw_X_train[col].unique()\n",
    "    test_categories = raw_X_test[col].unique()\n",
    "    all_categories[col] = np.union1d(train_categories, test_categories)\n",
    "\n",
    "\n",
    "# Generate summary tables for training and testing data\n",
    "train_summary = generate_summary_table(raw_X_train, raw_y_train, all_categories)\n",
    "test_summary = generate_summary_table(raw_X_test, raw_y_test, all_categories)\n",
    "test_summary.drop('Feature', axis = 1, inplace = True)\n",
    "\n",
    "# Combine the two tables side by side\n",
    "combined_summary = pd.concat([train_summary.add_suffix(' (Train)'), test_summary.add_suffix(' (Test)')], axis=1)\n",
    "\n",
    "# Display the combined summary table\n",
    "display(combined_summary)\n",
    "combined_summary.to_excel('../results/tables/SummaryTable.xlsx' ,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede55c48",
   "metadata": {},
   "source": [
    "# Univariable Analysis across ORN and non-ORN cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed25c91",
   "metadata": {},
   "source": [
    "Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(random_state=SEED, sample_posterior=True)\n",
    "df_impute = df.copy()\n",
    "imputed_values = imputer.fit_transform(df_impute[numerical_cols])\n",
    "df_impute[numerical_cols] = imputed_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3434ff",
   "metadata": {},
   "source": [
    "Check if expected frequency <= 5 --> if so, use Fishers Exact instead of Chi-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_list = []\n",
    "print('-'*40)\n",
    "print('Relevant Contingency Tables:')\n",
    "for col in df_impute:\n",
    "    if col in binary_cols:\n",
    "        contingency_table = pd.crosstab(df_impute[col], df_impute['ORN'])\n",
    "        _, _, _, expected_frequencies = chi2_contingency(contingency_table)\n",
    "        if (expected_frequencies < 5).any(): #True when at least 1 val is < 5\n",
    "            fish_list.append(col)\n",
    "            print(col)\n",
    "            print('-'*5)\n",
    "            print(expected_frequencies)\n",
    "print('-'*40)\n",
    "print('Features with an expected frequency <5 :')\n",
    "print(fish_list)\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get summary values to append to analysis values\n",
    "summary_df = generate_summary_table(df_impute.drop('ORN', axis = 1), df_impute.ORN, all_categories)\n",
    "summary_df.set_index('Feature', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b876cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odds_w_contingency(contingency_table):\n",
    "    # Extract cell values\n",
    "    a = contingency_table.loc[1, 1] if 1 in contingency_table.index and 1 in contingency_table.columns else 0\n",
    "    b = contingency_table.loc[1, 0] if 1 in contingency_table.index and 0 in contingency_table.columns else 0\n",
    "    c = contingency_table.loc[0, 1] if 0 in contingency_table.index and 1 in contingency_table.columns else 0\n",
    "    d = contingency_table.loc[0, 0] if 0 in contingency_table.index and 0 in contingency_table.columns else 0\n",
    "    \n",
    "    #Continuity correction\n",
    "    if a == 0 or b == 0 or c == 0 or d == 0:\n",
    "        a, b, c, d = a + 0.5, b + 0.5, c + 0.5, d + 0.5\n",
    "    odds_ratio = (a*d) / (b*c)\n",
    "    log_or = np.log(odds_ratio)\n",
    "    se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    z = 1.96 #For 95% CI\n",
    "    ci_lower = np.exp(log_or - z * se_log_or)\n",
    "    ci_upper = np.exp(log_or + z * se_log_or)\n",
    "    return odds_ratio, ci_lower, ci_upper\n",
    "\n",
    "def format_p_val(p_val):\n",
    "    ### Reformat p-val with string if low enough, otherwise round ###\n",
    "    if p_val < .0001:\n",
    "        return '<0.0001'\n",
    "    elif p_val < 0.05:\n",
    "        return sigfig.round(p_val, sigfigs=2)\n",
    "    else:\n",
    "        return round(p_val, 1)\n",
    "\n",
    "results = []\n",
    "for col in df_impute.drop('ORN', axis = 1): #Loop through all columns except ORN\n",
    "    ########## Binary Columns ##########\n",
    "    if col in binary_cols: \n",
    "        ### p-vals ###\n",
    "        contingency_table = pd.crosstab(df_impute[col], df_impute['ORN'])\n",
    "        if col in fish_list:\n",
    "        ## Fishers Exact for p-vals if expected freq < 5 \n",
    "            _, p_value = fisher_exact(contingency_table)\n",
    "        ## Otherwise chi-squared test \n",
    "        else: \n",
    "            _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        p_value = format_p_val(p_value)\n",
    "        ### Odds Ratios ###\n",
    "        odds_ratio, ci_lower, ci_upper = get_odds_w_contingency(contingency_table)\n",
    "        odds_conf = f'{odds_ratio:.2f} ({ci_lower:.2f}, {ci_upper:.2f})'\n",
    "        results.append((col, odds_conf, p_value))  \n",
    "    ########## Numerical Columns ##########\n",
    "    elif col in numerical_cols:\n",
    "    ### Mann-Whitney U test for p-vals ###\n",
    "        group1 = df_impute[df_impute['ORN'] == 0][col]\n",
    "        group2 = df_impute[df_impute['ORN'] == 1][col]\n",
    "        stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "        p_value = format_p_val(p_value)\n",
    "    ### Logit for ORs and CIs ###\n",
    "        model = Logit(df_impute['ORN'], add_constant(df_impute[col])).fit(disp=0)  \n",
    "        odds_ratio = np.exp(model.params[col])                         \n",
    "        conf_int = model.conf_int().loc[col].apply(np.exp)      \n",
    "        ci_lower = conf_int[0]\n",
    "        ci_upper = conf_int[1]\n",
    "        odds_conf = f'{odds_ratio:.2f} ({ci_lower:.2f}, {ci_upper:.2f})'\n",
    "        results.append((col.upper() + ', ± (SD)', odds_conf, p_value)) \n",
    "    ########## Nominal + Ordinal Columns ##########\n",
    "    elif col in nominal_cols + ordinal_cols:\n",
    "        # Get list of entries and value counts\n",
    "        entries = sorted(df_impute[col].unique()) \n",
    "        val_counts = df_impute[col].value_counts()\n",
    "        #Create subset onehot-encoded temporary df \n",
    "        temp_df = df_impute[[col, 'ORN']]\n",
    "        temp_df = pd.get_dummies(temp_df, columns=[col], drop_first=False, dtype=int)\n",
    "        #### Nominal ####\n",
    "        if col in nominal_cols:\n",
    "            # drop the entry with the highest freq and make that reference\n",
    "            max_freq_idx = val_counts.idxmax() #Entry with the highest frequency\n",
    "            temp_df.drop(f'{col}_{max_freq_idx}', axis = 1, inplace = True)\n",
    "        ### Ordinal ###\n",
    "        else: \n",
    "            # drop the entry with the lowest value and make that reference\n",
    "             temp_df.drop(f'{col}_{entries[0]}', axis = 1, inplace = True)\n",
    "            ### Logit for p-vals, ORs, and CIs ###\n",
    "        X = temp_df.drop('ORN', axis = 1)\n",
    "        y = temp_df['ORN']\n",
    "        X = add_constant(X)\n",
    "        model = Logit(y, X).fit(disp=0)\n",
    "        odds_ratios = np.exp(model.params) \n",
    "        conf_int = np.exp(model.conf_int())\n",
    "        p_values = model.pvalues\n",
    "        #Add variable header to df\n",
    "        results.append((col, '', ''))\n",
    "\n",
    "        # Loop through each possible entry of the feature\n",
    "        for entry in entries: \n",
    "            entry_name = f'{col}_{entry}' #Reformat to allow for indexing ex) 1.0 --> SITE_1.0\n",
    "            ### Nominal ###\n",
    "            if col in nominal_cols and entry == max_freq_idx: #Make highest freq entry the reference\n",
    "                results.append((f'{col} {entry}','Reference', 'Reference'))\n",
    "            ### Ordinal ###\n",
    "            elif col in ordinal_cols and entry == entries[0]: #Make lowest value entry the reference\n",
    "                results.append((f'{col} {entry}', 'Reference', 'Reference'))\n",
    "            #If not reference, get stat vals\n",
    "            else: \n",
    "                p_val_specific = format_p_val(p_values[entry_name])\n",
    "                odds_ratio = odds_ratios.loc[entry_name]\n",
    "                ci_lower = conf_int.loc[entry_name, 0]\n",
    "                ci_upper = conf_int.loc[entry_name, 1]\n",
    "                results.append((f'{col} {entry}', f'{odds_ratio:.2f} ({ci_lower:.2f}, {ci_upper:.2f})', p_val_specific))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Feature', 'Odds Ratio (95% CI)', 'P-Value',])\n",
    "results_df.set_index('Feature', inplace=True)\n",
    "final_table = summary_df.join(results_df, how='left').fillna('')\n",
    "display(final_table)\n",
    "final_table.to_excel('../results/tables/AnalysisTable.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
